{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from box import ConfigBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ehll\n"
     ]
    }
   ],
   "source": [
    "print(\"ehll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CONFIG_PATH\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "from src.constants import CONFIG_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 3] The system cannot find the path specified: 'Documents/Projects/ECommerce Customer Churn Prediction'\n",
      "c:\\Users\\asus\\Documents\\Projects\\ECommerce Customer Churn Prediction\\notebooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asus\\anaconda3\\envs\\venv\\Lib\\site-packages\\IPython\\core\\magics\\osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n"
     ]
    }
   ],
   "source": [
    "cd \"Documents/Projects/ECommerce Customer Churn Prediction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\asus\\\\Documents\\\\Projects\\\\ECommerce Customer Churn Prediction\\\\notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asus\\Documents\\Projects\\ECommerce Customer Churn Prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asus\\anaconda3\\envs\\venv\\Lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\asus\\\\Documents\\\\Projects\\\\ECommerce Customer Churn Prediction\\\\notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\asus\\\\Documents\\\\Projects\\\\ECommerce Customer Churn Prediction'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kaggle_dataset': <class 'str'>, 'gdrive_dataset': <class 'str'>, 'local_dataset': <class 'str'>, 'staging_dataset': <class 'str'>}\n"
     ]
    }
   ],
   "source": [
    "from src.constants import CONFIG_PATH,PARAMS_PATH,SCHEMA_PATH\n",
    "from src.utils.common import read_yaml\n",
    "from src.entity.data_ingestion import DataIngestionConfig\n",
    "\n",
    "print(DataIngestionConfig.__annotations__)\n",
    "\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self,config_path=CONFIG_PATH,params_path=PARAMS_PATH,schema_path=SCHEMA_PATH):\n",
    "        self.config=read_yaml(config_path),\n",
    "        self.params=read_yaml(params_path),\n",
    "        self.schema=read_yaml(schema_path)\n",
    "    def get_data_ingestion_config(self)->DataIngestionConfig:\n",
    "        config=self.config[0].data\n",
    "        data_ingestion=DataIngestionConfig(\n",
    "            kaggle_dataset=config.kaggle_source_path,\n",
    "            gdrive_dataset=config.gdrive_source_path,\n",
    "            local_dataset=config.raw,\n",
    "            staging_dataset=config.interim\n",
    "        )\n",
    "        return data_ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-21 18:01:25,844 : INFO: common: yaml file config\\config.yaml loaded successfully!]\n",
      "[2025-02-21 18:01:25,854 : INFO: common: yaml file config\\params.yaml loaded successfully!]\n",
      "[2025-02-21 18:01:25,859 : INFO: common: yaml file config\\schema.yaml loaded successfully!]\n",
      "DataIngestionConfig(kaggle_dataset='ankitverma2010/ecommerce-customer-churn-analysis-and-prediction', gdrive_dataset='https://drive.google.com/uc?id=1L33uQvZl0D2gNnu79tbJnjuX0ZAzr9_C', local_dataset='data/raw/', staging_dataset='data/interim/')\n"
     ]
    }
   ],
   "source": [
    "config=ConfigurationManager()\n",
    "print(config.get_data_ingestion_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asus\\anaconda3\\envs\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import kagglehub\n",
    "import shutil\n",
    "import gdown\n",
    "\n",
    "class DataIngestion:\n",
    "    def __init__(self):\n",
    "        self.config=ConfigurationManager().get_data_ingestion_config()\n",
    "    def download_kaggle_data(self):\n",
    "        path = kagglehub.dataset_download(self.config.kaggle_dataset,force_download=True)\n",
    "        if(os.path.exists(self.config.local_dataset+\"1/\") and os.path.isdir(self.config.local_dataset+\"1/\")):\n",
    "            shutil.rmtree(self.config.local_dataset+\"1/\")\n",
    "        shutil.move(path,self.config.local_dataset)\n",
    "        return pd.read_excel(self.config.local_dataset+'1/E Commerce Dataset.xlsx',sheet_name=1)\n",
    "    def gdrive_data(self):\n",
    "        #dowload data from public dataset hosted in gdrive\n",
    "        if(os.path.exists(self.config.local_dataset+\"EComm_gdrive_raw_data.xlsx\")):\n",
    "            os.remove(self.config.local_dataset+\"EComm_gdrive_raw_data.xlsx\")\n",
    "        gdown.download(self.config.gdrive_dataset,\"data/raw/EComm_gdrive_raw_data.xlsx\",quiet=False)\n",
    "        return pd.read_excel(self.config.local_dataset+'EComm_gdrive_raw_data.xlsx',sheet_name=1,engine='openpyxl')\n",
    "    def run(self):\n",
    "        kaggle_data_df=self.download_kaggle_data()\n",
    "        gdrive_data_df=self.gdrive_data()\n",
    "        \n",
    "        #Ensuring both datasets have same schema\n",
    "        total_rows=len(kaggle_data_df)\n",
    "        half_size=total_rows//2\n",
    "        \n",
    "        half_kaggle=kaggle_data_df.iloc[:half_size]\n",
    "        half_gdrive=gdrive_data_df.iloc[-half_size:]\n",
    "        \n",
    "        combined_data=pd.concat([half_kaggle,half_gdrive],ignore_index=True)\n",
    "        return combined_data.to_csv(self.config.staging_dataset+\"Combined_dataset_churn_prediction.csv\")\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-21 18:02:36,750 : INFO: common: yaml file config\\config.yaml loaded successfully!]\n",
      "[2025-02-21 18:02:36,757 : INFO: common: yaml file config\\params.yaml loaded successfully!]\n",
      "[2025-02-21 18:02:36,763 : INFO: common: yaml file config\\schema.yaml loaded successfully!]\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/ankitverma2010/ecommerce-customer-churn-analysis-and-prediction?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 532k/532k [00:01<00:00, 433kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1L33uQvZl0D2gNnu79tbJnjuX0ZAzr9_C\n",
      "To: c:\\Users\\asus\\Documents\\Projects\\ECommerce Customer Churn Prediction\\data\\raw\\EComm_gdrive_raw_data.xlsx\n",
      "100%|██████████| 556k/556k [00:00<00:00, 2.07MB/s]\n"
     ]
    }
   ],
   "source": [
    "ingestion=DataIngestion()\n",
    "df=ingestion.run()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.move(r\"C:\\Users\\asus\\.cache\\kagglehub\\datasets\\ankitverma2010\\ecommerce-customer-churn-analysis-and-prediction\\versions\\1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\asus\\\\Documents\\\\Projects\\\\ECommerce Customer Churn Prediction'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.constants import CONFIG_PATH,PARAMS_PATH,SCHEMA_PATH\n",
    "from src.utils.common import read_yaml\n",
    "from src.entity.data_ingestion import DataIngestionConfig\n",
    "from src.entity.data_storage import DataStorageConnectionConfig\n",
    "from src.entity.data_validation import DataValidationConfig\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self,config_path=CONFIG_PATH,params_path=PARAMS_PATH,schema_path=SCHEMA_PATH):\n",
    "        self.config=read_yaml(config_path),\n",
    "        self.params=read_yaml(params_path),\n",
    "        self.schema=read_yaml(schema_path)\n",
    "    def get_data_ingestion_config(self)->DataIngestionConfig:\n",
    "        config=self.config[0].data\n",
    "        data_ingestion=DataIngestionConfig(\n",
    "            kaggle_dataset=config.kaggle_source_path,\n",
    "            gdrive_dataset=config.gdrive_source_path,\n",
    "            local_dataset=config.raw,\n",
    "            staging_dataset=config.interim\n",
    "        )\n",
    "        return data_ingestion\n",
    "    def get_data_storage_config(self)->DataStorageConnectionConfig:\n",
    "        config=self.config[0].sql\n",
    "        data_connection=DataStorageConnectionConfig(\n",
    "            server_name=config.server_name,\n",
    "            db_name=config.db_name,\n",
    "            driver_name=config.driver_name,\n",
    "            trusted_conn=config.trusted_conn,\n",
    "            kaggle_table=config.kaggle_table_name,\n",
    "            gdrive_table=config.gdrive_table_name\n",
    "        )\n",
    "        return data_connection\n",
    "    def get_data_validation_config(self)->DataValidationConfig:\n",
    "        config=self.config[0].data_validation,\n",
    "        schema=self.schema.COLUMNS\n",
    "        print(config)\n",
    "        data_validation=DataValidationConfig(\n",
    "            data_source_path_kaggle=config[0].data_source_kaggle,\n",
    "            data_source_path_gdrive=config[0].data_source_gdrive,\n",
    "            Status_report=config[0].STATUS_REPORT_FILE,\n",
    "            all_schema=schema\n",
    "        )\n",
    "        return data_validation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-23 22:22:34,190 : INFO: common: yaml file config\\config.yaml loaded successfully!]\n",
      "[2025-02-23 22:22:34,197 : INFO: common: yaml file config\\params.yaml loaded successfully!]\n",
      "[2025-02-23 22:22:34,209 : INFO: common: yaml file config\\schema.yaml loaded successfully!]\n",
      "(ConfigBox({'data_source_kaggle': 'data/interim/kaggle/', 'data_source_gdrive': 'data/interim/gdrive/', 'STATUS_REPORT_FILE': 'uncommitted/data_docs/local_site/'}),)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataValidationConfig(data_source_path_kaggle='data/interim/kaggle/', data_source_path_gdrive='data/interim/gdrive/', Status_report='uncommitted/data_docs/local_site/', all_schema=ConfigBox({'CustomerID': 'int64', 'Tenure': 'float64', 'PreferredLoginDevice': 'object', 'CityTier': 'int64', 'WarehouseToHome': 'float64', 'PreferredPaymentMode': 'object', 'Gender': 'object', 'HourSpendOnApp': 'float64', 'NumberOfDeviceRegistered': 'int64', 'PreferedOrderCat': 'object', 'SatisfactionScore': 'int64', 'MaritalStatus': 'object', 'NumberOfAddress': 'int64', 'Complain': 'int64', 'OrderAmountHikeFromlastYear': 'float64', 'CouponUsed': 'float64', 'OrderCount': 'float64', 'DaySinceLastOrder': 'float64', 'CashbackAmount': 'float64'}))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storage=ConfigurationManager()\n",
    "storage.get_data_validation_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "from src.logger import logger\n",
    "\n",
    "class DataStorage:\n",
    "    def __init__(self):\n",
    "        self.config=ConfigurationManager().get_data_storage_config()\n",
    "        self.data_config=ConfigurationManager().get_data_ingestion_config()\n",
    "    def store_in_sql(self):\n",
    "        #1. Connecting with server\n",
    "        connection_string=f\"DRIVER={self.config.driver_name};SERVER={self.config.server_name};DATABASE={self.config.db_name};trusted_connection={self.config.trusted_conn}\"\n",
    "        create_kaggle_schema_query=\"\"\"IF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = 'kaggle')\n",
    "    EXEC('CREATE SCHEMA kaggle;');\"\"\"\n",
    "        create_gdrive_schema_query=\"\"\"IF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = 'gdrive')\n",
    "    EXEC('CREATE SCHEMA gdrive;');\"\"\"\n",
    "        conn=pyodbc.connect(connection_string)\n",
    "        try:\n",
    "            cursor=conn.cursor()\n",
    "            logger.info(\"Connection to DB Server was successfully established!\")\n",
    "            # 2. Create Schemas\n",
    "            cursor.execute(create_kaggle_schema_query)\n",
    "            cursor.execute(create_gdrive_schema_query)\n",
    "            # 3. Create Table \n",
    "            create_table_query_kaggle = f\"\"\"\n",
    "            IF OBJECT_ID('{self.config.kaggle_table}', 'U') IS NOT NULL\n",
    "                DROP TABLE {self.config.kaggle_table};\n",
    "            BEGIN\n",
    "                CREATE TABLE {self.config.kaggle_table} (\n",
    "                customer_id INT PRIMARY KEY,\n",
    "                churn_flag INT NOT NULL,\n",
    "                tenure INT NOT NULL,\n",
    "                preferred_device VARCHAR(50),\n",
    "                city_tier INT NOT NULL,\n",
    "                warehouse_to_home FLOAT,\n",
    "                preferred_payment VARCHAR(50),\n",
    "                gender VARCHAR(10),\n",
    "                hours_spent FLOAT,\n",
    "                num_devices INT,\n",
    "                preferred_order_category VARCHAR(100),\n",
    "                satisfaction_score INT CHECK (satisfaction_score BETWEEN 1 AND 10),\n",
    "                marital_status VARCHAR(20),\n",
    "                num_addresses INT,\n",
    "                complaints INT,\n",
    "                order_amount_hike FLOAT,\n",
    "                coupons_used INT,\n",
    "                order_count INT,\n",
    "                days_since_last_order INT,\n",
    "                cashback_amount FLOAT,\n",
    "                last_updated_date DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "            )\n",
    "            END\n",
    "            \"\"\"\n",
    "            create_table_query_gdrive = f\"\"\"\n",
    "            IF OBJECT_ID('{self.config.gdrive_table}', 'U') IS NOT NULL\n",
    "                DROP TABLE {self.config.gdrive_table};\n",
    "            BEGIN\n",
    "                CREATE TABLE {self.config.gdrive_table} (\n",
    "                customer_id INT PRIMARY KEY,\n",
    "                churn_flag INT NOT NULL,\n",
    "                tenure INT NOT NULL,\n",
    "                preferred_device VARCHAR(50),\n",
    "                city_tier INT NOT NULL,\n",
    "                warehouse_to_home FLOAT,\n",
    "                preferred_payment VARCHAR(50),\n",
    "                gender VARCHAR(10),\n",
    "                hours_spent FLOAT,\n",
    "                num_devices INT,\n",
    "                preferred_order_category VARCHAR(100),\n",
    "                satisfaction_score INT CHECK (satisfaction_score BETWEEN 1 AND 10),\n",
    "                marital_status VARCHAR(20),\n",
    "                num_addresses INT,\n",
    "                complaints INT,\n",
    "                order_amount_hike FLOAT,\n",
    "                coupons_used INT,\n",
    "                order_count INT,\n",
    "                days_since_last_order INT,\n",
    "                cashback_amount FLOAT,\n",
    "                last_updated_date DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "            )\n",
    "            END\n",
    "            \"\"\"\n",
    "            cursor.execute(create_table_query_kaggle)\n",
    "            cursor.execute(create_table_query_gdrive)\n",
    "            \n",
    "            #4. Load the data in kaggle and gdrive table\n",
    "            df=pd.read_csv(self.data_config.staging_dataset+\"Combined_dataset_churn_prediction.csv\")\n",
    "            for index,row in df.iterrows():\n",
    "                #print(row)\n",
    "                if(row[\"source\"]==\"kaggle\"):\n",
    "                    cursor.execute(\n",
    "                        f\"Insert into {self.config.kaggle_table}(customer_id,churn_flag,tenure,preferred_device,city_tier,warehouse_to_home,preferred_payment,gender,hours_spent,num_devices,preferred_order_category,satisfaction_score,marital_status,num_addresses,complaints,order_amount_hike,coupons_used,order_count,days_since_last_order,cashback_amount,last_updated_date) values(?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)\",row[\"CustomerID\"],row[\"Churn\"],row[\"Tenure\"],row[\"PreferredLoginDevice\"],row[\"CityTier\"],row[\"WarehouseToHome\"],row[\"PreferredPaymentMode\"],row[\"Gender\"],row[\"HourSpendOnApp\"],row[\"NumberOfDeviceRegistered\"],row[\"PreferedOrderCat\"],row[\"SatisfactionScore\"],row[\"MaritalStatus\"],row[\"NumberOfAddress\"],row[\"Complain\"],row[\"OrderAmountHikeFromlastYear\"],row[\"CouponUsed\"],row[\"OrderCount\"],row[\"DaySinceLastOrder\"],row[\"CashbackAmount\"],row[\"data_last_updated_dt\"])\n",
    "                    \n",
    "            conn.commit()\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-21 23:59:59,414 : INFO: common: yaml file config\\config.yaml loaded successfully!]\n",
      "[2025-02-21 23:59:59,419 : INFO: common: yaml file config\\params.yaml loaded successfully!]\n",
      "[2025-02-21 23:59:59,422 : INFO: common: yaml file config\\schema.yaml loaded successfully!]\n",
      "[2025-02-21 23:59:59,431 : INFO: common: yaml file config\\config.yaml loaded successfully!]\n",
      "[2025-02-21 23:59:59,436 : INFO: common: yaml file config\\params.yaml loaded successfully!]\n",
      "[2025-02-21 23:59:59,439 : INFO: common: yaml file config\\schema.yaml loaded successfully!]\n",
      "[2025-02-21 23:59:59,456 : INFO: 84454490: Connection to DB Server was successfully established!]\n"
     ]
    }
   ],
   "source": [
    "connection=DataStorage()\n",
    "connection.store_in_sql()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
